{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CountVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CountVectorizer provides a simple way to both tokenize a collection of text documents and build a vocabulary of known words, but also to encode new documents using that vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary------>>> {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "Tokens------>>> ['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
      "Vactor shape------>>> (1, 8)\n",
      "Generated Vector------->> [[1 1 1 1 1 1 1 2]]\n",
      "Word Dictionary ------>>> {'brown': 1, 'dog': 1, 'fox': 1, 'jumped': 1, 'lazy': 1, 'over': 1, 'quick': 1, 'the': 2}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "# list of text documents\n",
    "cv_text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "cv_vectorizer = CountVectorizer()\n",
    "# tokenize and build vocab\n",
    "cv_vectorizer.fit(cv_text)\n",
    "# summarize\n",
    "print(\"Vocabulary------>>>\",cv_vectorizer.vocabulary_)\n",
    "print(\"Tokens------>>>\",cv_vectorizer.get_feature_names())\n",
    "# encode document\n",
    "cv_vector = cv_vectorizer.transform(cv_text)\n",
    "# summarize encoded vector\n",
    "print(\"Vactor shape------>>>\",cv_vector.shape)\n",
    "print(\"Generated Vector------->>\",cv_vector.toarray())\n",
    "cv_vector_lst=list(*cv_vector.toarray())\n",
    "print(\"Word Dictionary ------>>>\",dict(list(zip(cv_vectorizer.get_feature_names(),cv_vector_lst))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The TfidfVectorizer will tokenize documents, learn the vocabulary and inverse document frequency weightings, and allow you to encode new documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary------>>> {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "Tokens------>>> ['brown', 'dog', 'fox', 'jumped', 'lazy', 'over', 'quick', 'the']\n",
      "Vactor shape------>>> (1, 8)\n",
      "Generated Vector------->> [[0.36388646 0.27674503 0.27674503 0.36388646 0.36388646 0.36388646\n",
      "  0.36388646 0.42983441]]\n",
      "IDF Word Dictionary ------>>> ------>>> {'brown': 1.6931471805599454, 'dog': 1.2876820724517808, 'fox': 1.2876820724517808, 'jumped': 1.6931471805599454, 'lazy': 1.6931471805599454, 'over': 1.6931471805599454, 'quick': 1.6931471805599454, 'the': 1.0}\n",
      "TF-IDF Score Word Dictionary ------>>> {'brown': 0.3638864554802418, 'dog': 0.27674502873103346, 'fox': 0.27674502873103346, 'jumped': 0.3638864554802418, 'lazy': 0.3638864554802418, 'over': 0.3638864554802418, 'quick': 0.3638864554802418, 'the': 0.4298344050159891}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "# create the transform\n",
    "tf_vectorizer = TfidfVectorizer()\n",
    "# tokenize and build vocab\n",
    "tf_vectorizer.fit(text)\n",
    "# summarize\n",
    "print(\"Vocabulary------>>>\",tf_vectorizer.vocabulary_)\n",
    "print(\"Tokens------>>>\",tf_vectorizer.get_feature_names())\n",
    "# encode document\n",
    "tf_vector = tf_vectorizer.transform([text[0]])\n",
    "# summarize encoded vector\n",
    "print(\"Vactor shape------>>>\",tf_vector.shape)\n",
    "print(\"Generated Vector------->>\",tf_vector.toarray())\n",
    "tf_vector_lst=list(*tf_vector.toarray())\n",
    "print(\"IDF Word Dictionary ------>>> ------>>>\",dict(list(zip(tf_vectorizer.get_feature_names(),tf_vectorizer.idf_))))\n",
    "print(\"TF-IDF Score Word Dictionary ------>>>\",dict(list(zip(tf_vectorizer.get_feature_names(),tf_vector_lst))))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The inverse document frequencies are calculated for each word in the vocabulary, assigning the lowest score of 1.0 to the most frequently observed word: “the” at index 7.\n",
    "2. Generated Vector is normalized score values between 0 and 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfidfTransformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Alternately, if you already have a learned CountVectorizer, you can use it with a TfidfTransformer to just calculate the inverse document frequencies and start encoding documents.\n",
    "2. With Tfidftransformer you will systematically compute word counts using CountVectorizer and then compute the Inverse Document Frequency (IDF) values and only then compute the Tf-idf scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IDF Word Dictionary ------>>> {'brown': 1.6931471805599454, 'dog': 1.2876820724517808, 'fox': 1.2876820724517808, 'jumped': 1.6931471805599454, 'lazy': 1.6931471805599454, 'over': 1.6931471805599454, 'quick': 1.6931471805599454, 'the': 1.0}\n",
      "TF-IDF Score Word Dictionary ------>>> {'brown': 0.3638864554802418, 'dog': 0.27674502873103346, 'fox': 0.27674502873103346, 'jumped': 0.3638864554802418, 'lazy': 0.3638864554802418, 'over': 0.3638864554802418, 'quick': 0.3638864554802418, 'the': 0.4298344050159891}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "tf_text = [\"The quick brown fox jumped over the lazy dog.\",\n",
    "        \"The dog.\",\n",
    "        \"The fox\"]\n",
    "cv_vectorizer=CountVectorizer() \n",
    "cv_word_count=cv_vectorizer.fit_transform(tf_text)\n",
    "tfidf_transformer=TfidfTransformer(smooth_idf=True,use_idf=True) \n",
    "tfidf_transformer.fit(cv_word_count)\n",
    "\n",
    "print(\"IDF Word Dictionary ------>>>\",dict(list(zip(cv_vectorizer.get_feature_names(),tfidf_transformer.idf_))))\n",
    "\n",
    "\n",
    "tf_idf_vector=tfidf_transformer.transform(cv_word_count)\n",
    "document_vector=tf_idf_vector[0]  ### Score for 1st Document\n",
    "tf_doc_vector_lst=list(*document_vector.toarray())\n",
    "print(\"TF-IDF Score Word Dictionary ------>>>\",dict(list(zip(cv_vectorizer.get_feature_names(),tf_doc_vector_lst))))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. The scores above make sense. The more common the word across documents, the lower its score and the more unique a word is to our first document."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summary:\n",
    "1. With Tfidfvectorizer on the contrary, you will do all three steps at once. Under the hood, it computes the word counts, IDF values, and Tf-idf scores all using the same dataset.\n",
    "2. If you need the term frequency (term count) vectors for different tasks, use Tfidftransformer.\n",
    "3. If you need to compute tf-idf scores on documents within your “training” dataset, use Tfidfvectorizer\n",
    "4. If you need to compute tf-idf scores on documents outside your “training” dataset, use either one, both will work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HashingVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. The HashingVectorizer class implements this approach that can be used to consistently hash words, then tokenize and encode documents as needed.\n",
    "2. Above Algorithms will require large vectors for encoding documents and impose large requirements on memory and slow down algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vactor shape------>>> (1, 20)\n",
      "Generated Vector------->> [[ 0.          0.          0.          0.          0.          0.33333333\n",
      "   0.         -0.33333333  0.33333333  0.          0.          0.33333333\n",
      "   0.          0.          0.         -0.33333333  0.          0.\n",
      "  -0.66666667  0.        ]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "# list of text documents\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "# create the transform\n",
    "hv_vectorizer = HashingVectorizer(n_features=20)\n",
    "# encode document\n",
    "hv_vector = hv_vectorizer.transform(text)\n",
    "# summarize encoded vector\n",
    "print(\"Vactor shape------>>>\",hv_vector.shape)\n",
    "print(\"Generated Vector------->>\",hv_vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
